tracktor:
  name: Tracktor++
  # Subfolder name in output/tracker/
  module_name: MOT17
  desription:
  seed: 12345
  # frcnn or fpn
  network: fpn

  # frcnn
  # obj_detect_weights: output/frcnn/res101/mot_2017_train/180k/res101_faster_rcnn_iter_180000.pth
  # obj_detect_config: output/frcnn/res101/mot_2017_train/180k/sacred_config.yaml

  # fpn
  obj_detect_model: output/faster_rcnn_fpn_training_mot_17/model_epoch_27.model
 

  reid_weights: output/tracktor/reid/res50-mot17-batch_hard/ResNet_iter_25245.pth
  reid_config: output/tracktor/reid/res50-mot17-batch_hard/sacred_config.yaml

  interpolate: False
  # compile video with: `ffmpeg -f image2 -framerate 15 -i %06d.jpg -vcodec libx264 -y movie.mp4 -vf scale=320:-1`
  write_images: False
  # dataset (look into tracker/datasets/factory.py)
  dataset: mot17_train_FRCNN17
  # [start percentage, end percentage], e.g., [0.0, 0.5] for train and [0.75, 1.0] for val split.
  frame_split: [0.0, 1.0]

  tracker:
    # FRCNN score threshold for detections
    detection_person_thresh: 0.5
    # FRCNN score threshold for keeping the track alive
    regression_person_thresh: 0.5
    # NMS threshold for detection
    detection_nms_thresh: 0.3
    # NMS theshold while tracking
    regression_nms_thresh: 0.6
    # motion model settings
    motion_model:
      enabled: False
      # average velocity over last n_steps steps
      n_steps: 1
      # if true, only model the movement of the bounding box center. If false, width and height are also modeled.
      center_only: True
    # DPM or DPM_RAW or 0, raw includes the unfiltered (no nms) versions of the provided detections,
    # 0 tells the tracker to use private detections (Faster R-CNN)
    public_detections: False
    # How much last appearance features are to keep
    max_features_num: 10
    # Do camera motion compensation
    do_align: False
    # Which warp mode to use (cv2.MOTION_EUCLIDEAN, cv2.MOTION_AFFINE, ...)
    warp_mode: cv2.MOTION_EUCLIDEAN
    # maximal number of iterations (original 50)
    number_of_iterations: 100
    # Threshold increment between two iterations (original 0.001)
    termination_eps: 0.00001
    # Use siamese network to do reid
    do_reid: True
    # How much timesteps dead tracks are kept and cosidered for reid
    inactive_patience: 10
    # How similar do image and old track need to be to be considered the same person
    reid_sim_threshold: 2.0
    # How much IoU do track and image need to be considered for matching
    reid_iou_threshold: 0.2

  reid:
    name: test
    module_name: reid
    desription:
    seed: 12345
    # smth like MOT_train, KITTI_train_Pedestrian
    db_train: mot_reid_small_train
    db_val: False
  
    model_args:
      # Recommended for loss: batch_all, batch_hard
      # Unstable, no guarantee they are working: weighted_triplet, cross_entropy
      loss: batch_hard
      margin: 0.2
      # Plot prec at k to tensorboard, 0 for off
      prec_at_k: 3
  
    solver:
      optim: Adam
      optim_args:
        lr: 0.0003
        weight_decay: 0.0000
  
    dataloader:
      # all targets with visibility lower than this are filtered out, for kitti set it to
      # a sequence with maximal [truncation, occlusion] levels
      vis_threshold: 0.3
      P: 18
      K: 4
      # limit maximum number of images per identity
      max_per_person: 1000
      crop_H: 256
      crop_W: 128
      # center: just a center crop, random: random crop and 0.5 horizontal flip probability
      transform: random
      normalize_mean:
        - 0.485
        - 0.456
        - 0.406
      normalize_std:
        - 0.229
        - 0.224
        - 0.225
  
    cnn:
      output_dim: 128
